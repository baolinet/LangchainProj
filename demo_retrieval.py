# æ¼”ç¤ºæ£€ç´¢ç³»ç»Ÿ - å±•ç¤ºå…·ä½“çš„æŸ¥è¯¢å’Œç»“æœ
from BGEM3FlagModel_compatible import OllamaBGEM3FlagModel
import numpy as np

def create_demo_knowledge_base():
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„çŸ¥è¯†åº“"""
    return [
        # AI/ML åŸºç¡€æ¦‚å¿µ
        "Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.",
        "Machine Learning is a method of data analysis that automates analytical model building using algorithms that iteratively learn from data.",
        "Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers to model complex patterns.",
        "Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains.",
        "Supervised learning uses labeled examples to learn a mapping from inputs to outputs, like classification and regression tasks.",
        "Unsupervised learning finds hidden patterns in data without labeled examples, including clustering and dimensionality reduction.",
        
        # NLP å’Œè¯­è¨€æŠ€æœ¯
        "Natural Language Processing (NLP) combines computational linguistics with statistical and machine learning methods to help computers understand human language.",
        "Large Language Models (LLMs) are neural networks trained on massive text datasets to understand and generate human-like text.",
        "BERT (Bidirectional Encoder Representations from Transformers) reads text bidirectionally to better understand context and meaning.",
        "GPT (Generative Pre-trained Transformer) is an autoregressive language model that generates text by predicting the next word in a sequence.",
        "Transformers use self-attention mechanisms to process sequences of data, revolutionizing natural language processing tasks.",
        "Word embeddings represent words as dense vectors in a continuous vector space where similar words have similar representations.",
        
        # æœç´¢å’Œæ£€ç´¢æŠ€æœ¯
        "Information retrieval is the activity of obtaining information system resources that are relevant to an information need from a collection of information resources.",
        "Vector search uses mathematical vectors to represent data points and finds similar items by calculating distances in high-dimensional space.",
        "Semantic search understands the intent and contextual meaning behind search queries rather than just matching exact keywords.",
        "Dense retrieval encodes queries and documents into dense vector representations using neural networks for semantic similarity matching.",
        "Sparse retrieval methods like TF-IDF and BM25 use statistical analysis of term frequencies and exact keyword matching.",
        "Vector databases are specialized storage systems optimized for storing, indexing, and querying high-dimensional vector embeddings.",
        
        # æ•°æ®å¤„ç†å’Œå­˜å‚¨
        "Big Data refers to datasets that are too large or complex for traditional data processing applications to handle effectively.",
        "Data preprocessing involves cleaning, transforming, and preparing raw data for analysis or machine learning model training.",
        "ETL (Extract, Transform, Load) is a data integration process that combines data from multiple sources into a single, consistent data store.",
        "Data warehouses are centralized repositories that store integrated data from multiple sources for business intelligence and analytics.",
        "Data lakes are storage repositories that hold vast amounts of raw data in its native format until it's needed for analysis.",
        "NoSQL databases provide flexible data models for storing and retrieving unstructured or semi-structured data at scale.",
        
        # äº‘è®¡ç®—å’Œæ¶æ„
        "Cloud computing delivers computing services including servers, storage, databases, networking, software, and analytics over the internet.",
        "Microservices architecture structures an application as a collection of loosely coupled services that communicate through APIs.",
        "Containerization packages software code with all its dependencies so the application runs quickly and reliably across computing environments.",
        "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "Serverless computing allows developers to build and run applications without thinking about servers or infrastructure management.",
        "APIs (Application Programming Interfaces) are sets of protocols and tools for building software applications and enabling communication between different systems."
    ]

def run_demo_queries():
    """è¿è¡Œæ¼”ç¤ºæŸ¥è¯¢"""
    print("ğŸš€ åˆå§‹åŒ– BGE-M3 æ£€ç´¢ç³»ç»Ÿæ¼”ç¤º")
    print("="*60)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = OllamaBGEM3FlagModel('bge-m3:567m', use_fp16=True)
    
    # åˆ›å»ºçŸ¥è¯†åº“
    docs = create_demo_knowledge_base()
    print(f"ğŸ“š çŸ¥è¯†åº“åŒ…å« {len(docs)} ä¸ªæ–‡æ¡£")
    
    # ç”Ÿæˆæ–‡æ¡£åµŒå…¥
    print("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡...")
    doc_embeddings = model.encode(docs)['dense_vecs']
    print(f"âœ… å®Œæˆï¼åµŒå…¥å‘é‡å½¢çŠ¶: {doc_embeddings.shape}")
    
    # æ¼”ç¤ºæŸ¥è¯¢åˆ—è¡¨
    demo_queries = [
        {
            "query": "What is the difference between machine learning and deep learning?",
            "description": "æ¯”è¾ƒæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«"
        },
        {
            "query": "How do vector databases work for similarity search?",
            "description": "å‘é‡æ•°æ®åº“å¦‚ä½•è¿›è¡Œç›¸ä¼¼æ€§æœç´¢"
        },
        {
            "query": "Explain the benefits of microservices architecture",
            "description": "è§£é‡Šå¾®æœåŠ¡æ¶æ„çš„ä¼˜åŠ¿"
        },
        {
            "query": "What are the main differences between data lakes and data warehouses?",
            "description": "æ•°æ®æ¹–å’Œæ•°æ®ä»“åº“çš„ä¸»è¦åŒºåˆ«"
        },
        {
            "query": "How do transformer models work in natural language processing?",
            "description": "Transformeræ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å·¥ä½œåŸç†"
        }
    ]
    
    print(f"\nğŸ” å¼€å§‹æ‰§è¡Œ {len(demo_queries)} ä¸ªæ¼”ç¤ºæŸ¥è¯¢...")
    
    for i, query_info in enumerate(demo_queries, 1):
        query = query_info["query"]
        description = query_info["description"]
        
        print(f"\n{'='*80}")
        print(f"ğŸ” æŸ¥è¯¢ {i}: {query}")
        print(f"ğŸ“ è¯´æ˜: {description}")
        print('='*80)
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = model.encode([query])['dense_vecs']
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = query_embedding @ doc_embeddings.T
        similarities = similarities[0]
        
        # è·å–å‰2ä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£
        top_indices = np.argsort(similarities)[::-1][:2]
        
        print("ğŸ¯ æ£€ç´¢ç»“æœ (Top 2):")
        
        for rank, idx in enumerate(top_indices, 1):
            similarity = similarities[idx]
            doc = docs[idx]
            
            # åˆ›å»ºç›¸ä¼¼åº¦å¯è§†åŒ–
            bar_length = int(similarity * 30)
            bar = "â–ˆ" * bar_length + "â–‘" * (30 - bar_length)
            
            print(f"\nğŸ“„ æ’å {rank}")
            print(f"ğŸ“Š ç›¸ä¼¼åº¦: {similarity:.4f} [{bar}] ({similarity*100:.1f}%)")
            print(f"ğŸ“– æ–‡æ¡£å†…å®¹:")
            print(f"   {doc}")
            
            # é«˜äº®å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰
            query_words = query.lower().split()
            doc_words = doc.lower().split()
            common_words = set(query_words) & set(doc_words)
            if common_words:
                print(f"ğŸ”‘ å…±åŒå…³é”®è¯: {', '.join(common_words)}")
    
    print(f"\n{'='*80}")
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ æç¤º: æ‚¨å¯ä»¥è¿è¡Œ 'python interactive_retrieval.py' è¿›è¡Œäº¤äº’å¼æŸ¥è¯¢")
    print('='*80)

if __name__ == "__main__":
    run_demo_queries()
