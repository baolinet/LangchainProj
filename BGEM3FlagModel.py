# ä½¿ç”¨ollamaä¸­çš„bge-m3æ¨¡å‹ - å…¼å®¹åŸBGEM3FlagModelæ¥å£
from BGEM3FlagModel_compatible import OllamaBGEM3FlagModel
import numpy as np

# ç›´æ¥æ›¿æ¢åŸæ¥çš„BGEM3FlagModelï¼Œæ¥å£å®Œå…¨å…¼å®¹
model = OllamaBGEM3FlagModel('bge-m3:567m', use_fp16=True)

# æ‰©å±•æ–‡æ¡£åº“ - åŒ…å«æ›´å¤šæŠ€æœ¯æ–‡æ¡£
docs = [
    # æœç´¢å’Œæ£€ç´¢ç›¸å…³
    "BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.",
    "BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document",
    "Dense retrieval uses neural networks to encode queries and documents into dense vector representations for semantic matching.",
    "Sparse retrieval methods like TF-IDF and BM25 rely on exact term matching and statistical word frequencies.",
    "Vector databases store high-dimensional embeddings and provide efficient similarity search capabilities.",
    "Semantic search understands the meaning and context of queries rather than just matching keywords.",

    # æœºå™¨å­¦ä¹ å’ŒAIç›¸å…³
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.",
    "Deep learning uses artificial neural networks with multiple layers to model and understand complex patterns in data.",
    "Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language.",
    "Transformer architecture revolutionized NLP by introducing self-attention mechanisms for processing sequential data.",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model that understands context from both directions.",
    "Large Language Models (LLMs) are AI systems trained on vast amounts of text data to generate human-like responses.",

    # æ•°æ®åº“å’Œå­˜å‚¨ç›¸å…³
    "Relational databases organize data into tables with rows and columns, using SQL for querying and management.",
    "NoSQL databases provide flexible data models for handling unstructured data, including document, key-value, and graph databases.",
    "Data warehouses are centralized repositories that store large amounts of structured data from multiple sources for analytics.",
    "ETL (Extract, Transform, Load) processes move data from source systems to data warehouses or other target systems.",
    "Data lakes store raw data in its native format, allowing for flexible processing and analysis later.",
    "OLAP (Online Analytical Processing) systems are designed for complex analytical queries and business intelligence.",

    # ç¼–ç¨‹å’Œå¼€å‘ç›¸å…³
    "Python is a high-level programming language known for its simplicity and extensive libraries for data science and AI.",
    "APIs (Application Programming Interfaces) define how different software components communicate with each other.",
    "REST (Representational State Transfer) is an architectural style for designing web services using HTTP methods.",
    "Microservices architecture breaks down applications into small, independent services that communicate over networks.",
    "Docker containers package applications and their dependencies into portable, lightweight units for deployment.",
    "Kubernetes orchestrates containerized applications, managing deployment, scaling, and operations automatically.",

    # äº‘è®¡ç®—å’ŒåŸºç¡€è®¾æ–½
    "Cloud computing delivers computing services over the internet, including servers, storage, databases, and software.",
    "AWS (Amazon Web Services) is a comprehensive cloud platform offering over 200 services for computing, storage, and databases.",
    "Serverless computing allows developers to run code without managing servers, automatically scaling based on demand.",
    "CDN (Content Delivery Network) distributes content across multiple geographic locations to improve performance.",
    "Load balancing distributes incoming network traffic across multiple servers to ensure high availability and performance.",
    "Auto-scaling automatically adjusts computing resources based on demand to maintain performance and optimize costs."
]

# æµ‹è¯•æŸ¥è¯¢ - æ¶µç›–ä¸åŒä¸»é¢˜
queries = [
    "What is the difference between dense and sparse retrieval?",
    "How do transformer models work in NLP?",
    "What are the benefits of using microservices architecture?",
    "Explain the concept of serverless computing",
    "What is the purpose of vector databases?"
]

print("=== æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿæ¼”ç¤º ===")
print(f"æ–‡æ¡£åº“å¤§å°: {len(docs)} ä¸ªæ–‡æ¡£")
print(f"æŸ¥è¯¢æ•°é‡: {len(queries)} ä¸ªæŸ¥è¯¢")
print()

# ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡
print("æ­£åœ¨ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡...")
docs_embeddings = model.encode(docs, batch_size=12, max_length=8192)['dense_vecs']
print(f"æ–‡æ¡£åµŒå…¥å‘é‡å½¢çŠ¶: {docs_embeddings.shape}")

# å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæ£€ç´¢
for i, query in enumerate(queries):
    print(f"\n{'='*60}")
    print(f"æŸ¥è¯¢ {i+1}: {query}")
    print('='*60)

    # ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡
    query_embedding = model.encode([query])['dense_vecs']

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = query_embedding @ docs_embeddings.T
    similarities = similarities[0]  # å–ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„ç»“æœ

    # è·å–æœ€ç›¸ä¼¼çš„2ä¸ªæ–‡æ¡£çš„ç´¢å¼•
    top_indices = np.argsort(similarities)[::-1][:2]

    print("ğŸ” æ£€ç´¢åˆ°çš„æœ€ç›¸å…³æ–‡æ¡£ (Top 2):")
    for rank, idx in enumerate(top_indices, 1):
        similarity_score = similarities[idx]
        print(f"\nğŸ“„ æ’å {rank} (ç›¸ä¼¼åº¦: {similarity_score:.4f}):")
        print(f"   {docs[idx]}")

print(f"\n{'='*60}")
print("æ£€ç´¢ä»»åŠ¡å®Œæˆï¼")
print('='*60)